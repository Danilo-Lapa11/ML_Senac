{"cells":[{"cell_type":"markdown","metadata":{"id":"Cu0vN-QIfahI"},"source":["# Redes Neurais Convolutionais"]},{"cell_type":"markdown","metadata":{"id":"3hfTHQfgfahK"},"source":["Na seção anterior, construímos e treinamos um modelo simples para classificar imagens ASL. O modelo foi capaz de aprender como classificar corretamente o conjunto de dados de treinamento com precisão muito alta, mas não teve um desempenho tão bom no conjunto de dados de validação. Esse comportamento de não generalizar bem para dados que não são de treinamento é chamado de [overfitting](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html) e nesta seção apresentaremos um tipo popular de modelo chamado [rede neural convolucional](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) que é especialmente bom para ler imagens e classificá-las."]},{"cell_type":"markdown","metadata":{"id":"u6Nw-o39fahL"},"source":["## Tarefas"]},{"cell_type":"markdown","metadata":{"id":"3u5gYYaBfahL"},"source":["* Prepare dados especificamente para uma CNN\n","* Crie um modelo CNN mais sofisticado, compreendendo uma maior variedade de camadas do modelo\n","* Treine um modelo CNN e observe seu desempenho"]},{"cell_type":"markdown","metadata":{"id":"ooGDTzq6fahM"},"source":["## Carregando e preparando os dados"]},{"cell_type":"markdown","metadata":{"id":"hex8Mx8KfahN"},"source":["A célula abaixo contém as técnicas de pré-processamento de dados, Execute-o antes de prosseguir:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6n2etmX9fahO"},"outputs":[],"source":["import tensorflow.keras as keras\n","import pandas as pd\n","\n","# Load in our data from CSV files\n","train_df = pd.read_csv(\"data/mnist/sign_mnist_train.csv\")\n","valid_df = pd.read_csv(\"data/mnist/sign_mnist_valid.csv\")\n","\n","# Separate out our target values\n","y_train = train_df['label']\n","y_valid = valid_df['label']\n","del train_df['label']\n","del valid_df['label']\n","\n","# Separate out our image vectors\n","x_train = train_df.values\n","x_valid = valid_df.values\n","\n","# Turn our scalar targets into binary categories\n","num_classes = 24\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_valid = keras.utils.to_categorical(y_valid, num_classes)\n","\n","# Normalize our image data\n","x_train = x_train / 255\n","x_valid = x_valid / 255"]},{"cell_type":"markdown","metadata":{"id":"fOmD22_7fahQ"},"source":["## Remodelando (Reshaping)  das Imagens para uma CNN"]},{"cell_type":"markdown","metadata":{"id":"GrFa7VDtfahQ"},"source":["As imagens individuais do nosso conjunto de dados estão no formato de longas listas de 784 pixels:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1699011912847,"user":{"displayName":"Danilo Wanderley Lapa","userId":"10188901348740999986"},"user_tz":180},"id":"pqmjUMV6fahR","outputId":"36429809-5fbb-4bff-d5d1-1a1e24e4f153"},"outputs":[{"data":{"text/plain":["((27455, 784), (7172, 784))"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["x_train.shape, x_valid.shape"]},{"cell_type":"markdown","metadata":{"id":"7EnwsZ7TfahR"},"source":["Neste formato, não temos todas as informações sobre quais pixels estão próximos uns dos outros. Por causa disso, não podemos aplicar convoluções que detectem recursos. Vamos remodelar nosso conjunto de dados para que fiquem no formato de 28x28 pixels. Isso permitirá que nossas convoluções associem grupos de pixels e detectem recursos importantes.\n","\n","Observe que para a primeira camada convolucional do nosso modelo, precisamos ter não apenas a altura e a largura da imagem, mas também o número de [canais de cores](https://www.photoshopessentials.com/essentials/rgb/) . Nossas imagens estão em tons de cinza, então teremos apenas 1 canal.\n","\n","Isso significa que precisamos converter a forma atual `(27455, 784)` para `(27455, 28, 28, 1)`. Por conveniência, podemos passar ao método [reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html#numpy.reshape) um `-1` para qualquer dimensão que desejarmos permanecem os mesmos, portanto:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dFEOljivfahS"},"outputs":[],"source":["x_train = x_train.reshape(-1,28,28,1)\n","x_valid = x_valid.reshape(-1,28,28,1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":499,"status":"ok","timestamp":1699012172652,"user":{"displayName":"Danilo Wanderley Lapa","userId":"10188901348740999986"},"user_tz":180},"id":"kzc9LxlRfahS","outputId":"20f0e8d7-4f3e-4975-ec9c-76d5202c7c3e"},"outputs":[{"data":{"text/plain":["(27455, 28, 28, 1)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["x_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":598,"status":"ok","timestamp":1699012186102,"user":{"displayName":"Danilo Wanderley Lapa","userId":"10188901348740999986"},"user_tz":180},"id":"gah5lzsXfahT","outputId":"73a064d7-c808-4724-e5ac-769ec61069fe"},"outputs":[{"data":{"text/plain":["(7172, 28, 28, 1)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["x_valid.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":472,"status":"ok","timestamp":1699012191766,"user":{"displayName":"Danilo Wanderley Lapa","userId":"10188901348740999986"},"user_tz":180},"id":"IwFAtwl0fahT","outputId":"7464bb04-4a90-4b30-c16e-1232c24d8b36"},"outputs":[{"data":{"text/plain":["((27455, 28, 28, 1), (7172, 28, 28, 1))"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["x_train.shape, x_valid.shape"]},{"cell_type":"markdown","metadata":{"id":"YKbYaOccfahT"},"source":["## Criando um Modelo Convolutional"]},{"cell_type":"markdown","metadata":{"id":"bXJf7seqfahT"},"source":["Hoje em dia, muitos cientistas de dados iniciam seus projetos pegando emprestadas propriedades de modelo de um projeto semelhante. Supondo que o problema não seja totalmente único, há uma grande chance de que as pessoas tenham criado modelos com bom desempenho e que sejam publicados em repositórios on-line como o [TensorFlow Hub](https://www.tensorflow.org/hub) e o [Catálogo NGC ](https://ngc.nvidia.com/catalog/models). Hoje, forneceremos um modelo que funcionará bem para esse problema.\n","\n","<img src=\"https://drive.google.com/uc?id=1WeMvcGJiRLmXCsp6kwozb-7slhHboooq\" width=180 />\n","\n","Abordamos muitos dos diferentes tipos de camadas na palestra e abordaremos todas elas aqui com links para sua documentação. Em caso de dúvida, leia a documentação oficial (ou pergunte ao [stackoverflow](https://stackoverflow.com/))."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tKGJEG1MfahU"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import (\n","    Dense,\n","    Conv2D,\n","    MaxPool2D,\n","    Flatten,\n","    Dropout,\n","    BatchNormalization,\n",")\n","\n","model = Sequential()\n","model.add(Conv2D(75, (3, 3), strides=1, padding=\"same\", activation=\"relu\", input_shape=(28, 28, 1))) # camada convulocional que soma os pixel\n","model.add(BatchNormalization())\n","model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n","\n","model.add(Conv2D(50, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n","model.add(Dropout(0.2))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n","\n","model.add(Conv2D(25, (3, 3), strides=1, padding=\"same\", activation=\"relu\"))\n","model.add(BatchNormalization()) #  normalização em lote dimensiona os valores nas camadas ocultas para melhorar o treinamento\n","model.add(MaxPool2D((2, 2), strides=2, padding=\"same\"))\n","model.add(Flatten()) # tansforma a matriz em um vetor\n","\n","model.add(Dense(units=512, activation=\"relu\"))\n","model.add(Dropout(0.3)) # desliga 30% dos neuronios de forma aleatoria\n","model.add(Dense(units=num_classes, activation=\"softmax\"))"]},{"cell_type":"markdown","metadata":{"id":"Vi3sOVBSfahU"},"source":["### [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)"]},{"cell_type":"markdown","metadata":{"id":"L9CBgAX6fahV"},"source":["<img src=\"https://drive.google.com/uc?id=1f4iuQX4IBOKW1bCAs9O_MTP-bqBlj-aQ\" largura=300 />\n","\n","Estas são nossas camadas convolucionais 2D. Kernels pequenos examinarão a imagem de entrada e detectarão recursos que são importantes para a classificação. As convoluções anteriores do modelo detectarão recursos simples, como linhas. As convoluções posteriores detectarão recursos mais complexos. Vejamos nossa primeira camada Conv2D:\n","```Python\n","model.add(Conv2D(75 , (3,3) , passadas = 1 , preenchimento = 'mesmo'...)\n","```\n","75 refere-se ao número de filtros que serão aprendidos. (3,3) refere-se ao tamanho desses filtros. Os avanços referem-se ao tamanho do passo que o filtro executará ao passar pela imagem. O preenchimento refere-se a se a imagem de saída criada a partir do filtro corresponderá ao tamanho da imagem de entrada."]},{"cell_type":"markdown","metadata":{"id":"Uia33jJnfahV"},"source":["### [BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) (Normalização em Lotes)"]},{"cell_type":"markdown","metadata":{"id":"_7bMyq71fahV"},"source":["Assim como a normalização de nossas entradas, a normalização em lote dimensiona os valores nas camadas ocultas para melhorar o treinamento. [Leia mais sobre isso em detalhes aqui](https://blog.paperspace.com/busting-the-myths-about-batch-normalization/)."]},{"cell_type":"markdown","metadata":{"id":"m-2jg3POfahW"},"source":["### [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)"]},{"cell_type":"markdown","metadata":{"id":"ive0XBTPfahW"},"source":["<img src=\"https://drive.google.com/uc?id=1V0KkSlK9mSrJJLSt6dZM-dN1ciUJJYH2\" width=300 />\n","\n","O Max pooling pega uma imagem e essencialmente a reduz para uma resolução mais baixa. Ele faz isso para ajudar o modelo a ser robusto à translação (objetos movendo-se de um lado para o outro) e também torna nosso modelo mais rápido."]},{"cell_type":"markdown","metadata":{"id":"CnBWEMaxfahW"},"source":["### [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout)"]},{"cell_type":"markdown","metadata":{"id":"fGJdMxNyfahW"},"source":["<img src=\"https://drive.google.com/uc?id=1Xd4HFauuDicUANuPu-sgTuUOjJ_Rhnat\" width=360 />\n","\n","Dropout é uma técnica para prevenir overfitting. O dropout seleciona aleatoriamente um subconjunto de neurônios e os desliga, para que eles não participem da propagação direta ou reversa naquela passagem específica. Isso ajuda a garantir que a rede seja robusta e redundante e não dependa de nenhuma área para encontrar respostas."]},{"cell_type":"markdown","metadata":{"id":"tapGTZ9NfahW"},"source":["### [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten)"]},{"cell_type":"markdown","metadata":{"id":"MkpwuO-TfahW"},"source":["Flatten pega a saída de uma camada que é multidimensional e a nivela em uma matriz unidimensional. A saída é chamada de vetor de características e será conectada à camada de classificação final."]},{"cell_type":"markdown","metadata":{"id":"aZhmmz69fahW"},"source":["### [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)"]},{"cell_type":"markdown","metadata":{"id":"Zn6e53mOfahX"},"source":["Já vimos camadas densas em nossos modelos anteriores. Nossa primeira camada densa (512 unidades) toma o vetor de recursos como entrada e aprende quais recursos contribuirão para uma classificação específica. A segunda camada densa (24 unidades) é a camada de classificação final que produz nossa previsão."]},{"cell_type":"markdown","metadata":{"id":"LE5-AfR6fahX"},"source":["\n","## Resumo do Modelo"]},{"cell_type":"markdown","metadata":{"id":"QPdk8DOPfahX"},"source":["Isso pode parecer muita informação, mas não se preocupe. Não é fundamental entender tudo agora para treinar modelos convolucionais com eficácia. Mais importante ainda, sabemos que eles podem ajudar na extração de informações úteis de imagens e podem ser usados em tarefas de classificação."]},{"cell_type":"markdown","metadata":{"id":"0H22JvBKfahX"},"source":["Aqui, resumimos o modelo que acabamos de criar. Observe como ele tem menos parâmetros treináveis que o modelo do notebook anterior:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":570,"status":"ok","timestamp":1699016053084,"user":{"displayName":"Danilo Wanderley Lapa","userId":"10188901348740999986"},"user_tz":180},"id":"qh9rB2rDfahX","outputId":"153d4c01-6477-4cea-958d-a2f3efed191d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_3 (Conv2D)           (None, 28, 28, 75)        750       \n","                                                                 \n"," batch_normalization_3 (Bat  (None, 28, 28, 75)        300       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_3 (MaxPoolin  (None, 14, 14, 75)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 14, 14, 50)        33800     \n","                                                                 \n"," dropout_2 (Dropout)         (None, 14, 14, 50)        0         \n","                                                                 \n"," batch_normalization_4 (Bat  (None, 14, 14, 50)        200       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_4 (MaxPoolin  (None, 7, 7, 50)          0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 7, 7, 25)          11275     \n","                                                                 \n"," batch_normalization_5 (Bat  (None, 7, 7, 25)          100       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d_5 (MaxPoolin  (None, 4, 4, 25)          0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_1 (Flatten)         (None, 400)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 512)               205312    \n","                                                                 \n"," dropout_3 (Dropout)         (None, 512)               0         \n","                                                                 \n"," dense_3 (Dense)             (None, 24)                12312     \n","                                                                 \n","=================================================================\n","Total params: 264049 (1.01 MB)\n","Trainable params: 263749 (1.01 MB)\n","Non-trainable params: 300 (1.17 KB)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"XTbpzgLdfahX"},"source":["## Compilando o Modelo"]},{"cell_type":"markdown","metadata":{"id":"BQNbCeH5fahY"},"source":["Compilaremos o modelo como antes:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FKB-v2GjfahY"},"outputs":[],"source":["model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"]},{"cell_type":"markdown","metadata":{"id":"bue-WIwafahY"},"source":["## Treinando o Modelo"]},{"cell_type":"markdown","metadata":{"id":"LMLYGlM0fahY"},"source":["Apesar da arquitetura do modelo muito diferente, o treinamento parece exatamente o mesmo. Execute a célula abaixo para treinar por 20 épocas e vamos ver se a precisão melhora:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":204384,"status":"ok","timestamp":1699016265171,"user":{"displayName":"Danilo Wanderley Lapa","userId":"10188901348740999986"},"user_tz":180},"id":"sk96bRTAfahY","outputId":"486f66c1-14d5-4461-ce5e-34bc7d73d965"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","858/858 [==============================] - 20s 10ms/step - loss: 0.3091 - accuracy: 0.9058 - val_loss: 0.4714 - val_accuracy: 0.8652\n","Epoch 2/20\n","858/858 [==============================] - 7s 8ms/step - loss: 0.0192 - accuracy: 0.9938 - val_loss: 0.3927 - val_accuracy: 0.8900\n","Epoch 3/20\n","858/858 [==============================] - 7s 8ms/step - loss: 0.0108 - accuracy: 0.9966 - val_loss: 0.1687 - val_accuracy: 0.9625\n","Epoch 4/20\n","858/858 [==============================] - 7s 8ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.4116 - val_accuracy: 0.9098\n","Epoch 5/20\n","858/858 [==============================] - 8s 9ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.2503 - val_accuracy: 0.9416\n","Epoch 6/20\n","858/858 [==============================] - 7s 8ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.1768 - val_accuracy: 0.9484\n","Epoch 7/20\n","858/858 [==============================] - 7s 8ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.1541 - val_accuracy: 0.9603\n","Epoch 8/20\n","858/858 [==============================] - 6s 7ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.2116 - val_accuracy: 0.9487\n","Epoch 9/20\n","858/858 [==============================] - 7s 8ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.2124 - val_accuracy: 0.9525\n","Epoch 10/20\n","858/858 [==============================] - 6s 7ms/step - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.2331 - val_accuracy: 0.9452\n","Epoch 11/20\n","858/858 [==============================] - 7s 8ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.4662 - val_accuracy: 0.9201\n","Epoch 12/20\n","858/858 [==============================] - 6s 7ms/step - loss: 1.0427e-04 - accuracy: 1.0000 - val_loss: 0.2557 - val_accuracy: 0.9416\n","Epoch 13/20\n","858/858 [==============================] - 7s 8ms/step - loss: 3.7616e-04 - accuracy: 0.9999 - val_loss: 0.1721 - val_accuracy: 0.9646\n","Epoch 14/20\n","858/858 [==============================] - 6s 7ms/step - loss: 5.1532e-04 - accuracy: 0.9999 - val_loss: 0.3491 - val_accuracy: 0.9430\n","Epoch 15/20\n","858/858 [==============================] - 7s 9ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.3144 - val_accuracy: 0.9363\n","Epoch 16/20\n","858/858 [==============================] - 6s 7ms/step - loss: 7.9512e-05 - accuracy: 1.0000 - val_loss: 0.2591 - val_accuracy: 0.9532\n","Epoch 17/20\n","858/858 [==============================] - 6s 7ms/step - loss: 1.4955e-04 - accuracy: 1.0000 - val_loss: 0.2469 - val_accuracy: 0.9537\n","Epoch 18/20\n","858/858 [==============================] - 6s 8ms/step - loss: 1.1954e-05 - accuracy: 1.0000 - val_loss: 0.2004 - val_accuracy: 0.9552\n","Epoch 19/20\n","858/858 [==============================] - 6s 7ms/step - loss: 1.8759e-04 - accuracy: 0.9999 - val_loss: 0.2365 - val_accuracy: 0.9558\n","Epoch 20/20\n","858/858 [==============================] - 7s 8ms/step - loss: 3.3363e-04 - accuracy: 0.9999 - val_loss: 0.3409 - val_accuracy: 0.9381\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x7ae2ee61d3c0>"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(x_train, y_train, epochs=20, verbose=1, validation_data=(x_valid, y_valid))"]},{"cell_type":"markdown","metadata":{"id":"7sGiDc1_fahY"},"source":["## Discussões dos resultados"]},{"cell_type":"markdown","metadata":{"id":"WORBhqOKfahY"},"source":["Parece que este modelo foi significativamente melhorado! A precisão do treinamento é muito alta e a precisão da validação também melhorou. Este é um ótimo resultado, pois tudo o que tivemos que fazer foi trocar por um novo modelo.\n","\n","Você deve ter notado a precisão da validação variando. Isto é uma indicação de que nosso modelo ainda não está generalizando perfeitamente. Felizmente, há mais que podemos fazer. Vamos falar sobre isso na próxima palestra."]},{"cell_type":"markdown","metadata":{"id":"7hGNIQb6fahZ"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"omki6mm0fahZ"},"source":["Nesta seção, utilizamos vários novos tipos de camadas para implementar uma CNN, que teve um desempenho melhor do que o modelo mais simples usado na última seção. Esperamos que o processo geral de criação e treinamento de um modelo com dados preparados esteja começando a se tornar ainda mais familiar."]},{"cell_type":"markdown","metadata":{"id":"MHmmxFqxfahZ"},"source":["## Limpando a memória\n","Antes de prosseguir, execute a seguinte célula para limpar a memória da GPU. Isso é necessário para passar para o próximo caderno."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Noz4lGWffahZ"},"outputs":[],"source":["#import IPython\n","#app = IPython.Application.instance()\n","#app.kernel.do_shutdown(True)"]},{"cell_type":"markdown","metadata":{"id":"QilTlVcrfahZ"},"source":["## Próximo"]},{"cell_type":"markdown","metadata":{"id":"XKTvBLF6fahZ"},"source":["Nas últimas seções você se concentrou na criação e no treinamento de modelos. Para melhorar ainda mais o desempenho, agora você voltará sua atenção para o *aumento de dados*, uma coleção de técnicas que permitirá que seus modelos sejam treinados com mais e melhores dados do que aqueles que você poderia ter originalmente à sua disposição."]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"11kbuN2ANLfGNx78SiDNZvi5eWyRsR_t_","timestamp":1699010618773},{"file_id":"1ShNzN2ywKZh0AeqbFQRBHKD7yTOTrKkW","timestamp":1698666609782},{"file_id":"17GNUm8-vpLuRqiydbfDiGR2oqoYR4SHn","timestamp":1664661280620}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
